{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import emoji\n",
    "import re\n",
    "import nltk\n",
    "# Esto sirve para configurar NLTK. La primera vez puede tardar un poco\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos la data\n",
    "\n",
    "Para cargar nuestra data, seguimos un proceso distinto al habitual debido a que est√° en formato JSON y contiene anidaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "with open ('australian_user_reviews.json', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        rows.append(ast.literal_eval(line))\n",
    "\n",
    "df_reviews = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "with open ('australian_users_items.json', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        rows.append(ast.literal_eval(line))\n",
    "\n",
    "df_items = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filas = []\n",
    "\n",
    "with open(\"output_steam_games.json\", encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        filas.append(json.loads((line))) # Como los tipo de valores no eran strings que es el tipo esperado por ast.litreal_eval, usamos json.loads\n",
    "\n",
    "df_games = pd.DataFrame(filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asi se veria nuestro DataFrame df_reviews. Observar que la columna reviews tiene una lista de diccionario por lo cual debemos de realizar el siguiente proceso para poder desanidar esta columna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desanidamos el dataFrame df_items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desanidada = []\n",
    "\n",
    "for index, row in df_items.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    items_count = row['items_count']\n",
    "    steam_id = row['steam_id']\n",
    "    user_url = row['user_url']\n",
    "    items = row['items']\n",
    "    \n",
    "    for i in items:   \n",
    "        new_row = {\n",
    "        'user_id': user_id,\n",
    "        'items_count': items_count,\n",
    "        'steam_id' : steam_id,\n",
    "        'user_url' : user_url,\n",
    "        'item_id': i.get('item_id', ''),\n",
    "        'item_name': i.get('item_name', ''),\n",
    "        'playtime_forever': i.get('playtime_forever', ''),\n",
    "        'playtime_2weeks': i.get('playtime_2weeks', '')\n",
    "        }\n",
    "        \n",
    "        data_desanidada.append(new_row)\n",
    "\n",
    "df_items_desanidada = pd.DataFrame(data_desanidada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desanidamos el dataFrame df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desanidada = []\n",
    "\n",
    "for index, row in df_reviews.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    user_url = row['user_url']\n",
    "    \n",
    "    reviews= row['reviews']\n",
    "    \n",
    "    \n",
    "    for i in reviews:   \n",
    "        new_row = {\n",
    "        'user_id': user_id,\n",
    "        'user_url':user_url,\n",
    "        \n",
    "        'funny': i.get('funny', ''),\n",
    "        'posted': i.get('posted', ''),\n",
    "        'last_edited': i.get('last_edited', ''),\n",
    "        'item_id': i.get('item_id', ''),\n",
    "        'helpful': i.get('helpful', ''),\n",
    "        'recommend': i.get('recommend', ''),\n",
    "        'review': i.get('review', '')\n",
    "        }\n",
    "        \n",
    "        data_desanidada.append(new_row)\n",
    "\n",
    "df_reviews_desanidada=pd.DataFrame(data_desanidada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de sentimiento\n",
    "Este analisis lo vamos a realizar para nuestro df_reviews_desanidada el cual tiene las reviews hecha por los usuarios de los video juegos. Para lo cual es necesario que nuestra columna reviews no tenga valores nulos o vacios para lo cual aplicamos una mascara que filtre todos los valores execto los vacios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_desanidada=df_reviews_desanidada[df_reviews_desanidada['review']!=\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de Emojis en la Columna de Review\n",
    "\n",
    "Nuestra columna de \"review\" contiene comentarios con emojis, por lo que es necesario filtrar estos emojis para realizar un tratamiento de datos m√°s efectivo. Esto nos permitir√° obtener informaci√≥n adicional para nuestro an√°lisis de sentimiento. Para llevar a cabo este proceso, vamos a utilizar una librer√≠a llamada `emoji`.\n",
    "\n",
    "Para comenzar, crearemos una funci√≥n llamada `emojis_count(x)`. Esta funci√≥n tomar√° como par√°metro (x) un texto y devolver√° una lista con los emojis presentes en dicho texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojis_count(x):\n",
    "    emojis = []\n",
    "    emojis_encontrados = emoji.emoji_list(x)\n",
    "    if emojis_encontrados == '':\n",
    "        return ''\n",
    "    for e in emojis_encontrados:\n",
    "        emojis.append(e['emoji'])\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a emplear una funci√≥n lambda para ingresar de manera individual cada uno de los textos de la columna \"review\" en la funci√≥n que creamos en el paso anterior. Esta acci√≥n nos devolver√° una lista; en caso de no encontrar emojis, la lista resultante estar√° vac√≠a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = df_reviews_desanidada['review'].apply(lambda x: emojis_count(x))\n",
    "emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente funci√≥n, `obtener_emojis_unicos(emojis)`, tiene como objetivo devolver una lista de emojis √∫nicos. Es decir, eliminar√° los emojis repetidos para obtener una lista que contenga cada emoji una sola vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_emojis_unicos(emojis):\n",
    "    return list(set(emojis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este siguiente paso, realizamos dos procesos:\n",
    "\n",
    "1. **Filtramos las listas vac√≠as** obtenidas en el paso anterior.\n",
    "2. Empleamos la funci√≥n creada anteriormente, `obtener_emojis_unicos(emojis)`, para **eliminar los emojis repetidos** de la lista resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = emojis[emojis.apply(lambda x: len(x) > 0)]\n",
    "emojis = emojis.apply(obtener_emojis_unicos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los emojis duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis.drop_duplicates(inplace=True)\n",
    "emojis.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ se encuentran todos los emojis √∫nicos que se hallaron en la columna de \"review\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asignaci√≥n de Palabras a Emojis y Signos de Puntuaci√≥n\n",
    "\n",
    "Creamos una lista de diccionarios en la que cada emoji encontrado tiene asignada una palabra espec√≠fica para su reemplazo. Adem√°s, solicitamos a ChatGPT una lista de emojis correspondientes a signos de puntuaci√≥n (:D, :p, :(), <3) a los cuales tambi√©n asignamos una palabra para su representaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_reemplazos = {\n",
    "    '‚úî': 'Approved ',\n",
    "    'üíØ': 'Perfect ',\n",
    "    '‚ôÄ': 'Woman ',\n",
    "    '‚ù§': 'Love ',\n",
    "    'üíã': 'Kiss ',\n",
    "    'üíÄ': 'Skull ',\n",
    "    '‚Ñ¢': 'Trademark ',\n",
    "    '¬Æ': 'Registered Trademark ',\n",
    "    '¬©': 'Copyright ',\n",
    "    '‚ò∫': 'Smile ',\n",
    "    '‚ô•': 'Heart ',\n",
    "    'üëå': 'Okay ',\n",
    "    'üëÄ': 'Eye ',\n",
    "    '‚åõ': 'Hourglass ',\n",
    "    '‚òπ': 'Sad ',\n",
    "    '‚òë': 'Checked ',\n",
    "    'üòä': 'Happiness ',\n",
    "    'üòÑ': 'Happiness ',\n",
    "    '‚óÄ': 'Left ',\n",
    "    ':D': 'laugh ',\n",
    "    ':3': 'smile',\n",
    "    'xD': 'laughloud',\n",
    "    '√¥‚Ä¢': 'oh',\n",
    "    ':c': 'adictive',\n",
    "    ':)': 'happy',\n",
    "    ':p': 'boring',\n",
    "    '^^': 'good',\n",
    "    'D=': 'pain',\n",
    "    ':(': 'sad',\n",
    "    'xd': 'happiness',\n",
    "    '=P': 'crazy',\n",
    "    '<3': 'heart',\n",
    "    ';>': 'blink',\n",
    "    '(Y)': 'thumbs up',\n",
    "    '---': 'enjoyable',\n",
    "    ',3': 'amazing',\n",
    "    'xt': 'unwanted',\n",
    "    '-' : 'recommended',\n",
    "    ':}': 'good',\n",
    "    'c:': 'happy',\n",
    "    ':v': 'want more',\n",
    "    'x3': 'cute face',\n",
    "    'XD': 'adictive',\n",
    "    '(y)': 'thumbs up',\n",
    "    ':P': 'bad',\n",
    "    ';0': 'laugh',\n",
    "    '=)': 'happy',\n",
    "    'gg': 'great game',\n",
    "    'GG': 'great game',\n",
    "    ';D': 'great game',\n",
    "    ':c': 'sad'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funci√≥n que reemplaza cada uno de los emojis por las palabras asignadas previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text):\n",
    "    for emoji, replacement in emoji_reemplazos.items():\n",
    "        text = text.replace(emoji, replacement)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la funci√≥n `replace_emojis(text)` a nuestra columna de \"review\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_desanidada['review']=df_reviews_desanidada['review'].apply(replace_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lisis de Sentimientos\n",
    "\n",
    "En este an√°lisis, vamos a utilizar la librer√≠a NLTK para determinar si las reviews contienen comentarios positivos, negativos o neutros. Para llevar a cabo este an√°lisis, utilizamos las siguientes librer√≠as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos la funci√≥n que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente funci√≥n realiza una serie de procesamientos sobre la columna 'review' de un DataFrame. Primero, itera a trav√©s de cada fila del DataFrame y realiza las siguientes acciones para cada 'review':\n",
    "\n",
    "1. Reemplaza los caracteres que no son letras por espacios.\n",
    "2. Convierte todas las letras a min√∫sculas.\n",
    "3. Tokeniza el texto para separar las palabras.\n",
    "4. Elimina las palabras con menos de un car√°cter.\n",
    "5. Remueve las palabras de parada (stopwords).\n",
    "6. Aplica un proceso de stemming para buscar la ra√≠z de las palabras.\n",
    "7. Finalmente, une nuevamente las palabras para reconstruir la review procesada y la agrega a una lista llamada `review_list`.\n",
    "\n",
    "Luego, crea una nueva columna en el DataFrame llamada 'review_normalizado' que contiene las reviews procesadas. Por √∫ltimo, imprime el DataFrame con las reviews procesadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "\n",
    "# Itera a trav√©s de las filas del DataFrame y su columna 'review'\n",
    "for index, row in df_reviews_desanidada.iterrows():\n",
    "    review = row['review']\n",
    "    \n",
    "    # Reemplazamos los caracteres que no sean letras por espacios\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \", str(review))\n",
    "    \n",
    "    # Convertimos todo a min√∫sculas\n",
    "    reviewr = review.lower()\n",
    "    \n",
    "    # Tokenizamos para separar las palabras\n",
    "    review = nltk.word_tokenize(review)\n",
    "    \n",
    "    # Eliminamos las palabras de menos de 1 caracter\n",
    "    review = [palabra for palabra in review if len(palabra) > 1]\n",
    "    \n",
    "    # Sacamos las Stopwords\n",
    "    review = [palabra for palabra in review if not palabra in stopwords]\n",
    "    \n",
    "    # Aplicamos la funci√≥n para buscar la ra√≠z de las palabras\n",
    "    review = [stemmer.stem(palabra) for palabra in review]\n",
    "    \n",
    "    # Por √∫ltimo, volvemos a unir el titular\n",
    "    review = \" \".join(review)\n",
    "    \n",
    "    # Agregamos el titular procesado a la lista\n",
    "    review_list.append(review)\n",
    "\n",
    "# Crear una nueva columna en el DataFrame con los titulares procesados\n",
    "df_reviews_desanidada['review_normalizado'] = review_list\n",
    "\n",
    "# Imprimir el DataFrame con los titulares procesados\n",
    "print(df_reviews_desanidada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Sentimientos\n",
    "\n",
    "Importamos la librer√≠a NLTK para analizar los sentimientos de nuestra columna de reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcional: C√≥digo Alternativo en Caso de Error al Cargar el Archivo Vader con NLTK\n",
    "\n",
    "En caso de que la librer√≠a NLTK no pueda cargar el archivo para Vader, se puede usar el siguiente c√≥digo como alternativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de An√°lisis de Sentimientos con NLTK\n",
    "\n",
    "A continuaci√≥n, presentaremos un breve ejemplo para mostrar c√≥mo la librer√≠a NLTK realiza el an√°lisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto a analizar\n",
    "x=\"it is a charming and beautiful produc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a instanciar el analizador de texto y realizar el an√°lisis del texto previamente mencionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()  # Instanciamos el analizados de texto\n",
    "resultado= sid.polarity_scores(x) # analizamos el texto\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que la salida del an√°lisis  incluye la probabilidad de que el texto tenga sentimiento positivo, negativo o neutro. En este caso, el sentimiento positivo tiene una probabilidad de 0.658.\n",
    "\n",
    "Tambi√©n podemos analizar la informaci√≥n proporcionada por el valor `compound`, el cual indica que si es mayor a 0, el texto tiene un sentimiento positivo; si es menor a 0, el sentimiento es negativo; y si es igual a 0, el sentimiento del texto es neutro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retomando el An√°lisis de Sentimiento de la Columna 'reviews'\n",
    "\n",
    "Vamos a aplicar una funci√≥n lambda a cada elemento de la columna 'review_normalizado' para realizar el an√°lisis de sentimiento. Esto nos devolver√° √∫nicamente el valor `compound`, el cual ser√° almacenado en una nueva columna llamada 'sentiment_analysis'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_desanidada['sentiment_analysis']=df_reviews_desanidada[\"review_normalizado\"].apply(lambda i: sid.polarity_scores(i)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A continuaci√≥n, presentaremos un breve ejemplo para mostrar c√≥mo la librer√≠a NLTK realiza el an√°lisis de sentimientos. **Es importante considerar** el orden en el que se realizan los reemplazos, ya que estos podr√≠an intercambiar los valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reemplazo de Valores en la Columna 'sentiment_analysis'\n",
    "\n",
    "Vamos a reemplazar los valores de la columna 'sentiment_analysis' con los par√°metros indicados en el archivo README del proyecto. Es esencial considerar el orden en el que se efect√∫an los reemplazos para evitar intercambios de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_desanidada.loc[df_reviews_desanidada['sentiment_analysis']>0,'sentiment_analysis']=2\n",
    "df_reviews_desanidada.loc[df_reviews_desanidada['sentiment_analysis']==0,'sentiment_analysis']=1\n",
    "df_reviews_desanidada.loc[df_reviews_desanidada['sentiment_analysis']<0,'sentiment_analysis']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversi√≥n de la Columna 'recommden' a Dummy\n",
    "\n",
    "Vamos a convertir la columna 'recommden' a una variable dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_recommend=pd.get_dummies(df_reviews_desanidada[\"recommend\"], prefix=\"recommend\").astype(int)\n",
    "df_reviews_desanidada=pd.concat([df_reviews_desanidada,dummy_recommend], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportamos la Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_desanidada.to_csv('reviews.csv', index=False)\n",
    "df_items_desanidada.to_csv('items.csv', index=False)\n",
    "df_games.to_csv('games.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
